{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# read csv\n",
    "path = \"Listing_Titles.tsv\"\n",
    "df = pd.read_csv(path, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"], quoting=csv.QUOTE_NONE)\n",
    "df[\"Record Number\"] = df[\"Record Number\"].astype(int)\n",
    "\n",
    "# read training csv\n",
    "train_path = \"Train_Tagged_Titles.tsv\"\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# maps of labels and ids\n",
    "labels2id = {train_df['Tag'].unique()[i - 1]: i for i in range(1, len(train_df['Tag'].unique()) + 1)}\n",
    "id2labels = {labels2id[i]: i for i in labels2id}\n",
    "\n",
    "# import model and tokenizer\n",
    "model = ...\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve results from each listing title\n",
    "def retrieve_results(tokenizer, text):\n",
    "    encoding = tokenizer(text, add_special_tokens=False, return_token_type_ids=False, return_attention_mask=False)\n",
    "    outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    # print(logits.shape)\n",
    "    predicted_label_classes = logits.argmax(-1)\n",
    "    # print(predicted_label_classes)\n",
    "    predicted_labels = [model.config.id2labels[id] for id in predicted_label_classes.squeeze().tolist()]\n",
    "    # print(predicted_labels)\n",
    "    tokens = [tokenizer.decode([id]) for id in encoding.input_ids.squeeze().tolist()]\n",
    "    # print tokens\n",
    "    return predicted_labels, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to decode BertTokenizer\n",
    "\n",
    "# check if part of string is subtoken\n",
    "def is_subtoken(word):\n",
    "    if word[:2] == \"##\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# decode tokens by combining subtokens\n",
    "def convert_token(token):\n",
    "    restored_token = []\n",
    "    token_list = token.split(\" \")\n",
    "    for i in range(len(token_list)):\n",
    "        \n",
    "        # check if element is start of value or if separated by '.'\n",
    "        if not is_subtoken(token_list[i]) and (i == 0 or token_list[i] == '.' or token_list[i-1] == '.'):\n",
    "            restored_token.append(token_list[i])\n",
    "        \n",
    "        # check if element is new word in value\n",
    "        elif not is_subtoken(token_list[i]):\n",
    "            restored_token.append(\" \" + token_list[i])\n",
    "\n",
    "        # element is a subtoken\n",
    "        else:\n",
    "            restored_token.append(token_list[i][2:])\n",
    "\n",
    "    # return combined value\n",
    "    return \"\".join(restored_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission dataframe\n",
    "df_submission = pd.DataFrame()\n",
    "record_numbers = []\n",
    "aspect_names = []\n",
    "aspect_values = []\n",
    "\n",
    "for i in range(5000, 30000):\n",
    "        title = df.iat[i,1]\n",
    "        labels, values = retrieve_results(tokenizer, title)\n",
    "        # record number\n",
    "        number = i+1\n",
    "        numbers = [number for label in labels]\n",
    "        record_numbers.extend(numbers)\n",
    "        aspect_names.extend(labels)\n",
    "        aspect_values.extend(values)\n",
    "\n",
    "# # decode all tokens in aspect_values\n",
    "# for i in range(len(aspect_values)):\n",
    "#         aspect_values[i] = convert_token(str(aspect_values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to pandas df\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['Record Number'] = record_numbers\n",
    "df_submission['Aspect Name'] = aspect_names\n",
    "df_submission['Aspect Value'] = aspect_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_submission' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cyrus/Desktop/eBay/BERT/retrieve_results.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cyrus/Desktop/eBay/BERT/retrieve_results.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m lastI \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cyrus/Desktop/eBay/BERT/retrieve_results.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m combineToken \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cyrus/Desktop/eBay/BERT/retrieve_results.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df_submission)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cyrus/Desktop/eBay/BERT/retrieve_results.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(df_submission\u001b[39m.\u001b[39miat[i, \u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cyrus/Desktop/eBay/BERT/retrieve_results.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(df_submission\u001b[39m.\u001b[39miat[i, \u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_submission' is not defined"
     ]
    }
   ],
   "source": [
    "# combine duplicate adjacent labels\n",
    "recNum = None\n",
    "prevName = None\n",
    "lastI = None\n",
    "combineToken = []\n",
    "for i in range(len(df_submission)):\n",
    "    num = int(df_submission.iat[i, 0])\n",
    "    name = str(df_submission.iat[i, 1])\n",
    "    val = str(df_submission.iat[i,2])\n",
    "    if num == recNum and name == prevName:\n",
    "        combineToken.append(val)\n",
    "        df_submission.iat[i,2] = 'drop'\n",
    "    else:\n",
    "        if len(combineToken) > 0:\n",
    "            combineToken.insert(0, str(df_submission.iat[lastI, 2]))\n",
    "            df_submission.iat[lastI, 2] = \" \".join(combineToken)\n",
    "            combineToken = []\n",
    "        prevName = name\n",
    "        recNum = num\n",
    "        lastI = i\n",
    "\n",
    "df_submission['Aspect Value'] = df_submission['Aspect Value'].astype(str)\n",
    "df_submission['Aspect Name'] = df_submission['Aspect Name'].astype(str)\n",
    "df_submission = df_submission[df_submission['Aspect Value']!='drop']\n",
    "df_submission = df_submission[df_submission['Aspect Name']!='0']\n",
    "df_submission = df_submission[df_submission['Aspect Value']!='[CLS]']\n",
    "df_submission = df_submission[df_submission['Aspect Value']!='[SEP]']\n",
    "# df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of values labeled 'obscure' and 'no tag'\n",
    "df_submission = df_submission[df_submission['Aspect Name'] != 'Obscure']\n",
    "df_submission = df_submission[df_submission['Aspect Name'] != 'No Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in submission format\n",
    "df_submission.to_csv(\"results.tsv\", sep=\"\\t\", encoding='utf-8', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eBay_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
